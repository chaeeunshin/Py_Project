---
title: Final exam
author: Chaeeun Shin
output: pdf_document
---
Installing packages
```{r}
if(!requireNamespace("tidyverse")) install.packages("tidyverse")
if(!requireNamespace("caret")) install.packages("caret")
if(!requireNamespace("neuralnet")) install.packages("neuralnet")
if(!requireNamespace("keras")) install.packages("keras")
if(!requireNamespace("randomForest")) install.packages("randomForest")
if(!requireNamespace("rpart")) install.packages("rpart")
if(!requireNamespace("rattle")) install.packages("rattle")
if(!requireNamespace("kernlab")) install.packages("kernlab")
```
Loading packages
```{r}
library(tidyverse)
library(caret)
library(neuralnet)
library(keras)
library(randomForest)
library(rpart)
library(rattle)
library(kernlab)
library(MASS)
library(tidyverse)
library(glmnet)
library(leaps)
library(ggplot2)
```
# Part I 
Loading data, splitting test and training data
```{r}
data <- read.csv('rest.csv')
data <- na.omit(data)
cat("There are", dim(data)[1], "observations left.")

set.seed(123)
training.samples <- data$y %>% 
  createDataPartition(p=0.75,list=FALSE)
train_data <- data[training.samples,]
test_data <- data[-training.samples,]

data$y <- as.factor(data$y)

nrow(train_data)
nrow(test_data)
```

Logistic model, prediction
```{r}
model <- glm(y~., train_data, family=binomial)

probabilities <- model %>% predict(test_data,type="response")
predicted_class_lm <- ifelse(probabilities>0.5,1,0)

confusionMatrix(factor(predicted_class_lm),factor(test_data$y))
```

Perception model, sse, 2 neurons
```{r}
set.seed(123)
model <- neuralnet(y~., data=train_data, hidden=2,err.fct="sse",linear.output=F)
plot(model, rep="best")

probabilities <- model %>% predict(test_data) %>% as.vector()
predicted_class_sse_2 <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_sse_2),factor(test_data$y),positive='1')
```

Perception model, sse, 3 neurons
```{r}
set.seed(123)
model <- neuralnet(y~., data=train_data, hidden=3,err.fct="sse",linear.output=F)
plot(model, rep="best")

probabilities <- model %>% predict(test_data) %>% as.vector()
predicted_class_sse_3 <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_sse_3),factor(test_data$y),positive='1')
```

Perception model, sse, 4 neurons
```{r}
set.seed(123)
model <- neuralnet(y~., data=train_data, hidden=4,err.fct="sse",linear.output=F)
plot(model, rep="best")

probabilities <- model %>% predict(test_data) %>% as.vector()
predicted_class_sse_4 <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_sse_4),factor(test_data$y),positive='1')
```

**Perception model with one hidden layer with 2 neuron refers the best accuracy**

Perception model, ce, 2 neurons
```{r}
set.seed(123)
model <- neuralnet(y~., data=train_data, hidden=2,err.fct="ce",linear.output=F)
plot(model, rep="best")

probabilities <- model %>% predict(test_data) %>% as.vector()
predicted_class_ce_2 <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_ce_2),factor(test_data$y),positive='1')
```

Perception model, ce, 3 neurons
```{r}
set.seed(123)
model <- neuralnet(y~., data=train_data, hidden=3,err.fct="ce",linear.output=F)
plot(model, rep="best")

probabilities <- model %>% predict(test_data) %>% as.vector()
predicted_class_ce_3 <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_ce_3),factor(test_data$y),positive='1')
```

Perception model, ce, 4 neurons
```{r}
set.seed(123)
model <- neuralnet(y~., data=train_data, hidden=4,err.fct="ce",linear.output=F)
plot(model, rep="best")

probabilities <- model %>% predict(test_data) %>% as.vector()
predicted_class_ce_4 <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_ce_4),factor(test_data$y),positive='1')
```

**Perception model with 2 and 4 neurons have almost same accuracy. (.9052)**

**SSE model provides the best accuracy for the test data.**

Fully grown tree
```{r}
model <- rpart(y~., data=train_data, control=rpart.control(cp=0))
fancyRpartPlot(model)
```

Pruned tree, prediction
```{r}
set.seed(123)
model2 <- train(y~., data=train_data, method="rpart",trControl=trainControl("cv",number=10),tuneLength=100)
plot(model2) 
model2$bestTune 
fancyRpartPlot(model2$finalModel)

probabilities <- predict(model2, newdata=test_data)
predicted_class_prunedtree <- ifelse(probabilities>0.5,1,0)

confusionMatrix(factor(predicted_class_prunedtree),factor(test_data$y))
```
randomForest OOB
```{r}
train_data$y <- factor(train_data$y)
test_data$y <- factor(test_data$y)

set.seed(123)
model <- train(y~., data=train_data, method="rf",trControl=trainControl("cv",number=10),importance=TRUE)

model$finalModel
```

Accuracy
```{r}
(1984+1168)/(1984+110+189+1168)
```

Sensitivity
```{r}
1168/(1168+189)
```

Specificity
```{r}
1984/(1984+110)
```

randomForest prediction 
```{r}
predicted_class_rf <- model %>% predict(test_data)
confusionMatrix(predicted_class_rf, test_data$y, positive='1')
```

Plot MeanDecreaseAccuracy
```{r}
varImpPlot(model$finalModel, type=1)
```

Plot MeanDecreaseGini
```{r}
varImpPlot(model$finalModel, type=2)
varImp(model,type=2)
```

Support vector machine
```{r}
set.seed(123)
model <- train(y~., data=train_data, method="svmRadial",trControl=trainControl("cv",number=10),tuneLength=4)
plot(model)
model$bestTune
```
Support vector machine prediction
```{r}
predicted_class_svmPoly <- predict(model, newdata=test_data)
confusionMatrix(predicted_class_svmPoly, test_data$y)
```
Ensemble classifier prediction
```{r}
pred <- cbind(predicted_class_sse_2, predicted_class_rf, predicted_class_prunedtree)
pred.m <- apply(pred,1,function(x) names(which.max(table(x))))

confusionMatrix(factor(pred.m), test_data$y, positive='1')
```
**Ensemble classifier is better than the five individual classifier.**

# Part II
```{r}
data <- read.csv("rest.csv")
data <- na.omit(data)
```

K-mean
```{r}
k.means.fit <- kmeans(data,2)
library(cluster)
clusplot(data,k.means.fit$cluster,main="2D representation of the Cluster solution",color=TRUE,shade=TRUE,labels=1,lines=0)
table(k.means.fit$cluster,data$y)
```
Accuracy
```{r}
(370+7)/(366+2422+1806+7)
```

Hierarchical: Ward
```{r}
d <- dist(data,method="euclidean")
H.fit <- hclust(d,method="ward.D")
plot(H.fit)
rect.hclust(H.fit,k=2,border="red")
groups <- cutree(H.fit,k=2)
clusters.ward <- factor(groups,levels=1:2,labels=c(0,1))
clusplot(data,groups,main="2D representation of the Cluster solution",color=TRUE,shade=TRUE,labels=2,lines=0)
table(data$y,clusters.ward)
```
Accuracy
```{r}
(2191+2)/(2191+597+1811+2)
```
Confusion matrix to compare the clustering results of the K-means and the Ward's method
```{r}
table(k.means.fit$cluster,clusters.ward)
```
Hierarchical: Average
```{r}
H.fit <- hclust(d,method="average")
plot(H.fit)
rect.hclust(H.fit,k=2,border="red")
groups <- cutree(H.fit,k=2)
clusters.average <- factor(groups,levels=1:2,labels=c(0,1))
clusplot(data,groups,main="2D representation of the Cluster solution",color=TRUE,shade=TRUE,labels=2,lines=0)
table(data$y,clusters.average)
```
Accuracy
```{r}
(2786+1)/(1812+2+1+2786)
```
**H.Average>H.Ward>K.means based on their accuracy on confusion matrix**

```{r}
library(devtools)
library(ggbiplot)
```

```{r}
data.pca <- prcomp(data,center=TRUE,scale. = TRUE)
summary(data.pca)
```

```{r}
ggbiplot(data.pca)
```

```{r}
ggbiplot(data.pca, ellipse = TRUE, groups = data$y)
```

```{r}
data.pca$rotation[,1]
```

**The first PC contains 0.1674 of whole information of the original variable.**

#Part III
```{r}
if(!requireNamespace("tidyquant")) install.packages("tidyquant")
if(!requireNamespace("magrittr")) install.packages("magrittr")
if(!requireNamespace("tensorflow")) install.packages("tensorflow")
if(!requireNamespace("zoo")) install.packages("zoo")
library(tidyquant)
library(magrittr)
library(zoo)
```

```{r}
data <- read.csv('restt.csv')
data <- na.omit(data)
ggplot(data,aes(x=Date,y=Close)) + geom_line()
```

```{r}
data$min_lagged <- lag(data$Low)
data$max_lagged <- log(data$High)
data$Close_norm <- (data$Close-data$min_lagged)/(data$max_lagged-data$min_lagged)

model_data <- matrix(data$Close_norm[-1])

knitr::kable(tail(model_data,10))
```

```{r}
train_data <- head(model_data,-10)
test_data <- tail(model_data,10)
cat(dim(train_data)[1], 'days are divided into the training set')
```
