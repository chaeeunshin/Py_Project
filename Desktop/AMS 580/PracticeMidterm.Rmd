---
title: Practice Midterm
output: pdf_document
---
```{r}
library(tidyverse)
library(dplyr)
library(cloudml)
library(randomForest)
library(caret)
library(caTools)
library(rpart)
library(rattle)
library(openxlsx)
library(neuralnet)
```

#Part 1.
#1. 

```{r}
enigma <- read.csv("Enigma.csv")
enigma$x13_1 <- ifelse(enigma$x13==1,1,0) # dummy variables
enigma$x13_2 <- ifelse(enigma$x13==2,1,0)
enigma <- subset(enigma, select = -c(13))
enigma <- na.omit(enigma)
cat("There are", nrow(enigma), "observations left.")
```

```{r}
set.seed(123)
training_samples <- enigma$y %>% 
  createDataPartition(p=0.75, list=FALSE)
train.enigma <- enigma[training_samples,]
test.enigma <- enigma[-training_samples,]
nrow(train.enigma)
nrow(test.enigma)
```

#2.
```{r}
set.seed(123)
model <- neuralnet(y~., data=train.enigma, hidden=3, err.fct = "ce", act.fct="logistic",linear.output=FALSE)
plot(model, rep="best")
```

```{r}
probabilities <- predict(model, test.enigma)
predicted.y <- ifelse(probabilities>0.5,1,0)
(c1 <- confusionMatrix(factor(predicted.y),factor(test.enigma$y),positive="1"))
```

#3.
```{r}
train.enigma$y <- as.factor(train.enigma$y)
test.enigma$y <- as.factor(test.enigma$y)
set.seed(123)
fulltree <- train(y~.,data=train.enigma,method="rpart",trControl=trainControl("cv",number=10),tuneLength=100)
plot(fulltree)
fulltree$bestTune
fancyRpartPlot(fulltree$finalModel)
```

```{r}
prunedtree <- train(y~.,data=train.enigma, method="rpart",trControl=trainControl("cv",number=10))
plot(prunedtree)
prunedtree$bestTune
fancyRpartPlot(prunedtree$finalModel)
```

```{r}
pred <- predict(prunedtree, newdata=test.enigma)
(cm2 <- confusionMatrix(pred,factor(test.enigma$y),positive="1"))
```

#Part 2
#1.
```{r}
mystery <- read.csv("Mystery.csv")
mystery$x15 <- ifelse(mystery$x15=='y',1,0)
mean <- mean(mystery$y)
sd <- sd(mystery$y)
mystery <- data.frame(scale(mystery)) #data normalization
mystery <- na.omit(mystery)
cat("There are", nrow(mystery), "observations left.")
```

```{r}
set.seed(123)
training.samples <- mystery$y %>%
  createDataPartition(p=0.75,list=FALSE)
train.mystery <- mystery[training.samples,]
test.mystery <- mystery[-training.samples,]
nrow(train.mystery)
nrow(test.mystery)
```

#2.
```{r}
library(tensorflow)
library(keras)
train_x <- as.matrix(subset(train.mystery,select=-y))
train_y <- as.matrix(subset(train.mystery,select=y))
test_x <- as.matrix(subset(test.mystery,select=-y))
test_y <- as.matrix(subset(test.mystery,select=y))
```

```{r}
set.seed(123)
model3 <- keras_model_sequential()
model3 %>% layer_dense(units=3,activation='relu',input_shape=c(15)) %>% layer_dense(units=1,activation="linear")
model3 %>% compile(loss="mse",optimizer="adam",metrics="mse")
summary(model3)
history = model3 %>% fit(train_x,train_y, epochs=50, batch_size=8,validation_split=0.1)
plot(history)
```

```{r}
preds <- predict(model3, test_x)
RMSE(test.mystery$y,preds)
RMSE(test.mystery$y*sd+mean,preds*sd+mean)
```

```{r}
detach(package:keras, unload=TRUE)
detach(package:tensorflow,unload=TRUE)

model4 <- train(y~.,data=train.mystery, method="rpart",trControl=trainControl("cv",number=10),tuneLength=100)
plot(model4)
model4$bestTune
fancyRpartPlot(model4$finalModel)
predictions <- model4 %>% predict(test.mystery)
RMSE(test.mystery$y,predictions)
RMSE(test.mystery$y*sd+mean,predictions*sd+mean)
```
